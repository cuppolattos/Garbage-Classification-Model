{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24d230af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5b7427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 15515 files belonging to 12 classes.\n",
      "Using 12412 files for training.\n",
      "Found 15515 files belonging to 12 classes.\n",
      "Using 3103 files for validation.\n",
      "Kelas yang ditemukan: ['battery', 'biological', 'brown-glass', 'cardboard', 'clothes', 'green-glass', 'metal', 'paper', 'plastic', 'shoes', 'trash', 'white-glass']\n"
     ]
    }
   ],
   "source": [
    "DATASET_PATH = 'dataset'\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "VALIDATION_SPLIT = 0.2\n",
    "\n",
    "# training\n",
    "train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATASET_PATH,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset=\"training\",\n",
    "    seed=123,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "# validasi\n",
    "validation_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "    DATASET_PATH,\n",
    "    validation_split=VALIDATION_SPLIT,\n",
    "    subset=\"validation\",\n",
    "    seed=123,\n",
    "    image_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "class_names = train_dataset.class_names\n",
    "print(\"Kelas yang ditemukan:\", class_names)\n",
    "num_classes = len(class_names)\n",
    "\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_dataset = train_dataset.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "validation_dataset = validation_dataset.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c3fdb75",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_augmentation = tf.keras.Sequential([\n",
    "    tf.keras.layers.RandomFlip(\"horizontal_and_vertical\"),\n",
    "    tf.keras.layers.RandomRotation(0.2),\n",
    "    tf.keras.layers.RandomZoom(0.2),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07aecc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " sequential (Sequential)     (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 224, 224, 3)       0         \n",
      "                                                                 \n",
      " mobilenetv2_1.00_224 (Funct  (None, 7, 7, 1280)       2257984   \n",
      " ional)                                                          \n",
      "                                                                 \n",
      " global_average_pooling2d (G  (None, 1280)             0         \n",
      " lobalAveragePooling2D)                                          \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 1280)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 12)                15372     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,273,356\n",
      "Trainable params: 15,372\n",
      "Non-trainable params: 2,257,984\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "IMG_SHAPE = IMAGE_SIZE + (3,)\n",
    "\n",
    "\n",
    "base_model = MobileNetV2(input_shape=IMG_SHAPE,\n",
    "                         include_top=False,\n",
    "                         weights='imagenet')\n",
    "\n",
    "base_model.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    tf.keras.Input(shape=IMG_SHAPE),\n",
    "    data_augmentation,\n",
    "    tf.keras.layers.Lambda(tf.keras.applications.mobilenet_v2.preprocess_input),\n",
    "    base_model,\n",
    "    GlobalAveragePooling2D(),\n",
    "    Dropout(0.2),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer=Adam(learning_rate=0.0001),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15e6e28a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting RngReadAndSkip cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting Bitcast cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting StatelessRandomUniformV2 cause there is no registered converter for this op.\n",
      "WARNING:tensorflow:Using a while_loop for converting ImageProjectiveTransformV3 cause there is no registered converter for this op.\n",
      "388/388 [==============================] - ETA: 0s - loss: 1.5511 - accuracy: 0.5317\n",
      "Epoch 1: val_loss improved from inf to 0.89484, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 640s 2s/step - loss: 1.5511 - accuracy: 0.5317 - val_loss: 0.8948 - val_accuracy: 0.7780\n",
      "Epoch 2/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.7906 - accuracy: 0.7772\n",
      "Epoch 2: val_loss improved from 0.89484 to 0.57600, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 543s 1s/step - loss: 0.7906 - accuracy: 0.7772 - val_loss: 0.5760 - val_accuracy: 0.8598\n",
      "Epoch 3/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.5951 - accuracy: 0.8273\n",
      "Epoch 3: val_loss improved from 0.57600 to 0.45161, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 531s 1s/step - loss: 0.5951 - accuracy: 0.8273 - val_loss: 0.4516 - val_accuracy: 0.8856\n",
      "Epoch 4/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.4937 - accuracy: 0.8535\n",
      "Epoch 4: val_loss improved from 0.45161 to 0.38581, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 882s 2s/step - loss: 0.4937 - accuracy: 0.8535 - val_loss: 0.3858 - val_accuracy: 0.8975\n",
      "Epoch 5/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.4470 - accuracy: 0.8696\n",
      "Epoch 5: val_loss improved from 0.38581 to 0.34438, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 791s 2s/step - loss: 0.4470 - accuracy: 0.8696 - val_loss: 0.3444 - val_accuracy: 0.9104\n",
      "Epoch 6/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.4089 - accuracy: 0.8779\n",
      "Epoch 6: val_loss improved from 0.34438 to 0.31601, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 615s 2s/step - loss: 0.4089 - accuracy: 0.8779 - val_loss: 0.3160 - val_accuracy: 0.9194\n",
      "Epoch 7/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.3831 - accuracy: 0.8836\n",
      "Epoch 7: val_loss improved from 0.31601 to 0.29425, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 582s 2s/step - loss: 0.3831 - accuracy: 0.8836 - val_loss: 0.2943 - val_accuracy: 0.9239\n",
      "Epoch 8/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.3565 - accuracy: 0.8911\n",
      "Epoch 8: val_loss improved from 0.29425 to 0.27819, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 577s 1s/step - loss: 0.3565 - accuracy: 0.8911 - val_loss: 0.2782 - val_accuracy: 0.9227\n",
      "Epoch 9/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.3478 - accuracy: 0.8916\n",
      "Epoch 9: val_loss improved from 0.27819 to 0.26678, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 608s 2s/step - loss: 0.3478 - accuracy: 0.8916 - val_loss: 0.2668 - val_accuracy: 0.9265\n",
      "Epoch 10/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.3369 - accuracy: 0.8912\n",
      "Epoch 10: val_loss improved from 0.26678 to 0.25638, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 561s 1s/step - loss: 0.3369 - accuracy: 0.8912 - val_loss: 0.2564 - val_accuracy: 0.9278\n",
      "Epoch 11/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.3115 - accuracy: 0.9024\n",
      "Epoch 11: val_loss improved from 0.25638 to 0.24971, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 569s 1s/step - loss: 0.3115 - accuracy: 0.9024 - val_loss: 0.2497 - val_accuracy: 0.9288\n",
      "Epoch 12/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.3180 - accuracy: 0.9006\n",
      "Epoch 12: val_loss improved from 0.24971 to 0.24401, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 556s 1s/step - loss: 0.3180 - accuracy: 0.9006 - val_loss: 0.2440 - val_accuracy: 0.9288\n",
      "Epoch 13/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.2997 - accuracy: 0.9080\n",
      "Epoch 13: val_loss improved from 0.24401 to 0.23564, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 544s 1s/step - loss: 0.2997 - accuracy: 0.9080 - val_loss: 0.2356 - val_accuracy: 0.9304\n",
      "Epoch 14/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.2926 - accuracy: 0.9082\n",
      "Epoch 14: val_loss improved from 0.23564 to 0.23031, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 590s 2s/step - loss: 0.2926 - accuracy: 0.9082 - val_loss: 0.2303 - val_accuracy: 0.9326\n",
      "Epoch 15/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.2890 - accuracy: 0.9063\n",
      "Epoch 15: val_loss improved from 0.23031 to 0.22599, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 605s 2s/step - loss: 0.2890 - accuracy: 0.9063 - val_loss: 0.2260 - val_accuracy: 0.9343\n",
      "Epoch 16/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.2825 - accuracy: 0.9118\n",
      "Epoch 16: val_loss improved from 0.22599 to 0.22565, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 609s 2s/step - loss: 0.2825 - accuracy: 0.9118 - val_loss: 0.2257 - val_accuracy: 0.9346\n",
      "Epoch 17/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.2790 - accuracy: 0.9128\n",
      "Epoch 17: val_loss improved from 0.22565 to 0.21788, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 604s 2s/step - loss: 0.2790 - accuracy: 0.9128 - val_loss: 0.2179 - val_accuracy: 0.9346\n",
      "Epoch 18/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.2715 - accuracy: 0.9166\n",
      "Epoch 18: val_loss improved from 0.21788 to 0.21749, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 576s 1s/step - loss: 0.2715 - accuracy: 0.9166 - val_loss: 0.2175 - val_accuracy: 0.9349\n",
      "Epoch 19/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.2662 - accuracy: 0.9179\n",
      "Epoch 19: val_loss improved from 0.21749 to 0.21485, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 544s 1s/step - loss: 0.2662 - accuracy: 0.9179 - val_loss: 0.2149 - val_accuracy: 0.9375\n",
      "Epoch 20/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.2585 - accuracy: 0.9177\n",
      "Epoch 20: val_loss improved from 0.21485 to 0.21176, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 585s 2s/step - loss: 0.2585 - accuracy: 0.9177 - val_loss: 0.2118 - val_accuracy: 0.9352\n",
      "Epoch 21/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.2578 - accuracy: 0.9159\n",
      "Epoch 21: val_loss improved from 0.21176 to 0.20739, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 600s 2s/step - loss: 0.2578 - accuracy: 0.9159 - val_loss: 0.2074 - val_accuracy: 0.9375\n",
      "Epoch 22/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.2537 - accuracy: 0.9191\n",
      "Epoch 22: val_loss improved from 0.20739 to 0.20472, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 607s 2s/step - loss: 0.2537 - accuracy: 0.9191 - val_loss: 0.2047 - val_accuracy: 0.9391\n",
      "Epoch 23/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.2551 - accuracy: 0.9158\n",
      "Epoch 23: val_loss improved from 0.20472 to 0.20458, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 531s 1s/step - loss: 0.2551 - accuracy: 0.9158 - val_loss: 0.2046 - val_accuracy: 0.9368\n",
      "Epoch 24/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.2421 - accuracy: 0.9227\n",
      "Epoch 24: val_loss improved from 0.20458 to 0.20091, saving model to models\\garbage_classifier_v2_earlystop.h5\n",
      "388/388 [==============================] - 679s 2s/step - loss: 0.2421 - accuracy: 0.9227 - val_loss: 0.2009 - val_accuracy: 0.9401\n",
      "Epoch 25/25\n",
      "388/388 [==============================] - ETA: 0s - loss: 0.2460 - accuracy: 0.9187\n",
      "Epoch 25: val_loss did not improve from 0.20091\n",
      "388/388 [==============================] - 634s 2s/step - loss: 0.2460 - accuracy: 0.9187 - val_loss: 0.2013 - val_accuracy: 0.9397\n"
     ]
    }
   ],
   "source": [
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=3, \n",
    "    verbose=1, \n",
    "    restore_best_weights=True\n",
    ")\n",
    "\n",
    "MODEL_PATH = 'models/garbage_classifier_v3_earlystop.h5'\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    MODEL_PATH, \n",
    "    monitor='val_loss', \n",
    "    save_best_only=True, \n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=validation_dataset,\n",
    "    epochs=50,\n",
    "    callbacks=[early_stopping, model_checkpoint]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d58d12",
   "metadata": {},
   "source": [
    "### PLOT ACCURACY & LOSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a77914db",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'history' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m acc \u001b[38;5;241m=\u001b[39m \u001b[43mhistory\u001b[49m\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      2\u001b[0m val_acc \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m      3\u001b[0m loss \u001b[38;5;241m=\u001b[39m history\u001b[38;5;241m.\u001b[39mhistory[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mloss\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mNameError\u001b[0m: name 'history' is not defined"
     ]
    }
   ],
   "source": [
    "acc = history.history['accuracy']\n",
    "val_acc = history.history['val_accuracy']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(acc, label='Training Accuracy')\n",
    "plt.plot(val_acc, label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(loss, label='Training Loss')\n",
    "plt.plot(val_loss, label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.show()\n",
    "\n",
    "model.save(MODEL_PATH)\n",
    "print(f\"Model berhasil disimpan! {MODEL_PATH}\")\n",
    "\n",
    "print(\"\\nMemulai evaluasi model pada data validasi...\")\n",
    "results = model.evaluate(validation_dataset, verbose=1)\n",
    "\n",
    "print(\"\\n\" + \"=\"*30)\n",
    "print(\"   HASIL EVALUASI AKHIR\")\n",
    "print(\"=\"*30)\n",
    "print(f\"Loss Akhir      : {results[0]:.4f}\")\n",
    "print(f\"Akurasi Akhir   : {results[1] * 100:.2f}%\")\n",
    "print(\"=\"*30)\n",
    "\n",
    "import pickle\n",
    "with open('models/class_names.pkl', 'wb') as f:\n",
    "    pickle.dump(class_names, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "garbage_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
